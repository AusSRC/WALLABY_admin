{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "935cdfea",
   "metadata": {},
   "source": [
    "# WALLABY public data release notebook\n",
    "\n",
    "This notebook is intended to support with exporting the WALLABY source and kinematic data tables and associated products to be ingested into a public archive (CADC and CASDA). It is intended that the user of this notebook will be a member of the WALLABY project team, or a member of the WALLABY TWG7 group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd0cc98-4b05-447c-aa33-3d27a84251d1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5630c448-e003-4ae0-9efc-bfef293568a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import getpass\n",
    "import requests\n",
    "import getpass\n",
    "import pyvo as vo\n",
    "from pyvo.auth import authsession, securitymethods\n",
    "import numpy as np\n",
    "from astropy.io import ascii\n",
    "from astropy.io.votable import from_table, parse_single_table\n",
    "from astropy.table import vstack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a7a9ff-8084-4ad1-9d2c-d0d8aa62dab6",
   "metadata": {},
   "source": [
    "### Authenticate\n",
    "\n",
    "<span style=\"font-weight: bold; color: #FF0000;\">⚠ Update the cell below with your username and enter your password</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4220e4-57ba-49e5-be19-983caf994019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter WALLABY user username and password\n",
    "\n",
    "username = 'wallaby_user'\n",
    "password = getpass.getpass('Enter your password')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02fa60a-d21d-483f-82a0-2862322e57e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect with TAP service\n",
    "\n",
    "URL = \"https://wallaby.aussrc.org/tap\"\n",
    "auth = vo.auth.AuthSession()\n",
    "auth.add_security_method_for_url(URL, vo.auth.securitymethods.BASIC)\n",
    "auth.credentials.set_password(username, password)\n",
    "tap = vo.dal.TAPService(URL, session=auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a27862-d912-4d6f-ad9d-82ccbb3aad9b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9800389-c6bd-42f3-89eb-a484dcf62118",
   "metadata": {},
   "source": [
    "# 1. Decide Release\n",
    "\n",
    "Determine which internal releases you would like to bundle in this public data release. You will also need to set a name for this public data release."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bb2bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all tags\n",
    "\n",
    "query = \"SELECT * FROM wallaby.tag\"\n",
    "votable = tap.search(query)\n",
    "table = votable.to_table()\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8bc157-fa15-4266-b52a-22b9e1127475",
   "metadata": {},
   "source": [
    "<span style=\"font-weight: bold; color: #FF0000;\">⚠ Update the cell below. Add tags to the list for release, and update `release_name` variable</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e633128b-9114-4937-8d29-7ec0294bbfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of tags\n",
    "tags = ['Hydra DR2']\n",
    "\n",
    "# Release name\n",
    "release_name_raw = \"WALLABY Test PDR\"\n",
    "release_name = release_name_raw.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc709bf5-0ced-485a-a750-6f50229579dd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8af3de",
   "metadata": {},
   "source": [
    "# 2. Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0d9803-88d3-4734-bd38-2202c26d2d47",
   "metadata": {},
   "source": [
    "## Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac09df6-aa7a-4069-83c1-01946cfe2057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve catalog as Astropy table\n",
    "\n",
    "query = \"\"\"SELECT d.*, ivo_string_agg(t.name || ': ' || t.description, '; ') AS tags, ivo_string_agg(c.comment, '; ') AS comments\n",
    "        FROM wallaby.detection d\n",
    "        LEFT JOIN wallaby.tag_detection td ON d.id = td.detection_id \n",
    "        LEFT JOIN wallaby.tag t ON t.id = td.tag_id\n",
    "        LEFT JOIN wallaby.comment c ON d.id = c.detection_id\n",
    "        WHERE t.name IN ('Internal Data Release', '$TAG_NAME')\n",
    "        GROUP BY d.id\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807033dd-7a35-4c13-953f-6e93238b0436",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "table = None\n",
    "for idx, tag_name in enumerate(tags):\n",
    "    q = query.replace('$TAG_NAME', tag_name)\n",
    "    result = tap.search(q)\n",
    "    if idx == 0:\n",
    "        table = result.to_table()\n",
    "        table['SRCTR'] = tag_name.replace(' ', '_').replace('DR', 'TR')\n",
    "    else:\n",
    "        new_table = result.to_table()\n",
    "        new_table['SRCTR'] = tag_name.replace(' ', '_').replace('DR', 'TR')\n",
    "        table = vstack([table, new_table])\n",
    "\n",
    "table = table[0:20]\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afd86a0-764f-461c-875b-24c5cdbe0a60",
   "metadata": {},
   "source": [
    "## Modifying the catalog\n",
    "\n",
    "There are some additional columns and calculated properties that are required for the release. The column metadata (e.g. UCDs, units, description etc as required to conform with VO standards) also need to be included for these additional columns. These include:\n",
    "\n",
    "| Column | Description |\n",
    "| --- | --- |\n",
    "| `qflag` |  |\n",
    "| `kflag` | column to indicate whether or not there is a kinematic model associated with the detection |\n",
    "| `team_release` | Column with the release name |\n",
    "| `f_sum_corr` | |\n",
    "| `err_f_sum_corr` | |\n",
    "| `dist_h` | |\n",
    "| `log_m_hi_corr` | Uses `v_est` and `dist_est` which are calculated properties |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa29d2a-ae43-4253-81ee-39ab102e6d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table corrections\n",
    "\n",
    "rest_freq = 1.42040575179E+09\n",
    "c = 2.9979245e8\n",
    "H0 = 70.0\n",
    "\n",
    "write_table = table.copy()\n",
    "write_table['name'] = table['source_name']\n",
    "write_table['qflag'] = table['flag']\n",
    "write_table['kflag'] = np.zeros(len(table['flag']))\n",
    "write_table['team_release'] = release_name_raw\n",
    "write_table['f_sum_corr'] = table['f_sum'] / 10.0 ** (0.0285 * np.log10(table['f_sum'])**3.0 -0.439 * np.log10(table['f_sum'])**2.0 + 2.294 * np.log10(table['f_sum']) - 4.097)\n",
    "write_table['err_f_sum_corr'] = table['err_f_sum'] / table['f_sum'] * write_table['f_sum_corr']\n",
    "write_table['v_est'] = ((rest_freq - table['freq']) / table['freq'] * c / 1000.0)\n",
    "write_table['dist_h'] = write_table['v_est'] / H0\n",
    "write_table['log_m_hi'] = np.log10(49.7 * write_table['dist_h']**2.0 * table['f_sum'])\n",
    "write_table['log_m_hi_corr'] = np.log10(49.7 * write_table['dist_h']**2.0 * write_table['f_sum_corr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833b22d6-fa82-46e7-8da9-8400ff61ee9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove certain columns from the astropy table\n",
    "\n",
    "write_table.remove_columns(['id', 'run_id', 'instance_id', 'access_url', 'access_format', 'source_name', 'flag', 'v_est', 'l', 'b', 'v_rad', 'v_opt', 'v_app', 'tags', 'SRCTR'])\n",
    "votable = from_table(write_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd385ff-dbfb-4f59-9fcd-c354d6591be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update derived quantity columns of votable\n",
    "\n",
    "f_sum_corr_field = votable.get_field_by_id('f_sum_corr')\n",
    "f_sum_corr_field.ucd = \"phot.flux;meta.main\"\n",
    "f_sum_corr_field.unit = \"Jy*Hz\"\n",
    "f_sum_corr_field.description = \"The integrated flux within 3D source mask statistically corrected to match single dish observations\"\n",
    "\n",
    "err_f_sum_corr_field = votable.get_field_by_id('err_f_sum_corr')\n",
    "err_f_sum_corr_field.ucd = \"stat.error;phot.flux\"\n",
    "err_f_sum_corr_field.unit = \"Jy*Hz\"\n",
    "err_f_sum_corr_field.description = \"Statistical uncertainty of the single dish corrected integrated flux\"\n",
    "\n",
    "dist_h_field = votable.get_field_by_id('dist_h')\n",
    "dist_h_field.ucd = \"pos.distance\"\n",
    "dist_h_field.unit = \"Mpc\"\n",
    "dist_h_field.description = \"Local Hubble distance derived from the barycentric source frequency\"\n",
    "\n",
    "log_m_hi_field = votable.get_field_by_id('log_m_hi')\n",
    "log_m_hi_field.ucd = \"phys.mass\"\n",
    "log_m_hi_field.unit = \"log10(Msol)\"\n",
    "log_m_hi_field.description = \"The estimated log10 mass of the cube using f_sum and freq\"\n",
    "\n",
    "log_m_hi_corr_field = votable.get_field_by_id('log_m_hi_corr')\n",
    "log_m_hi_corr_field.ucd = \"phys.mass\"\n",
    "log_m_hi_corr_field.unit = \"log10(Msol)\"\n",
    "log_m_hi_corr_field.description = \"The estimated log10 mass of the cube using f_sum_corr and freq\"\n",
    "\n",
    "qflag_field = votable.get_field_by_id('qflag')\n",
    "qflag_field.datatype = \"double\"\n",
    "qflag_field.ucd = \"meta.code.qual\"\n",
    "qflag_field.description = \"Quality flag\"\n",
    "\n",
    "kflag_field = votable.get_field_by_id('kflag')\n",
    "kflag_field.datatype = \"double\"\n",
    "kflag_field.ucd = \"meta.code\"\n",
    "kflag_field.description = \"Kinematic model flag\"\n",
    "\n",
    "comments_field = votable.get_field_by_id('comments')\n",
    "comments_field.datatype = \"char\"\n",
    "comments_field.ucd = \"meta.note\"\n",
    "comments_field.description = \"Comments on individual sources\"\n",
    "\n",
    "team_release_field = votable.get_field_by_id('team_release')\n",
    "team_release_field.datatype = \"char\"\n",
    "team_release_field.ucd = \"meta.dataset;meta.main\"\n",
    "team_release_field.description = \"Internal team release identifier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319e31d3-a350-4132-b3e3-42e2003a006c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(write_table.columns)\n",
    "print(len(write_table.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9b253d-c993-4894-9361-baa0934a3b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download catalog table\n",
    "\n",
    "votable.version = '1.3'\n",
    "votable_filename = f'{release_name}_SourceCatalogue.xml'\n",
    "votable.to_xml(votable_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5711c1-928d-4a93-918e-355d5949a735",
   "metadata": {},
   "source": [
    "## Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9179ec6d-9ed5-468d-9411-cc4e5746b081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful function for downloading table products (requires authentication)\n",
    "\n",
    "def download_products(row, products_filename, chunk_size=8192):\n",
    "    \"\"\"Download products for a row of the table (a detection entry)\n",
    "    \n",
    "    \"\"\"\n",
    "    name = row['source_name']\n",
    "    access_url = row['access_url']\n",
    "    votable = parse_single_table(access_url)\n",
    "    product_table = votable.to_table()\n",
    "    url = product_table[product_table['description'] == 'SoFiA-2 Detection Products'][0]['access_url']\n",
    "    with requests.get(url, auth=(username, password), stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(products_filename, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                f.write(chunk)\n",
    "    print(f'Downloaded completed for {name}')\n",
    "    return\n",
    "\n",
    "def download_table_products(table, directory, chunk_size=8192):\n",
    "    \"\"\"Download WALLABY products from ADQL queried table\n",
    "\n",
    "    \"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.mkdir(directory)\n",
    "    print(f'Saving products to {directory}')\n",
    "    for row in table:\n",
    "        name = row['source_name']\n",
    "        products_filename = os.path.join(directory, f'{name}.tar')\n",
    "        download_products(row, products_filename, chunk_size)\n",
    "    print('Downloads complete')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278faf65-85e4-445b-b51c-dcd2540832bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write output products for a source\n",
    "\n",
    "download_table_products(table[0:20], release_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596ccb3c-140b-4321-8ce2-b31bc17da0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update product files\n",
    "\n",
    "import tarfile\n",
    "import glob\n",
    "from astropy.io import fits\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213d2d0d-0f0a-49a6-9b87-8a5bc74195ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_hdu(ra, dec, frequency):\n",
    "    \"\"\"Create dummy pixel hdu element (TBA)\n",
    "    \n",
    "    \"\"\"\n",
    "    return\n",
    "\n",
    "def spectrum_to_fits(f_in, f_cube, f_out, ra, dec, frequency):\n",
    "    \"\"\"Convert the SoFiA-2 output spectrum .txt file to a .fits file for public release\n",
    "    Writes a HDUList object with:\n",
    "        - dummy pixel (fits metadata from the accompanying _cube.fits file\n",
    "        - fits binary table with spectra.txt content\n",
    "        \n",
    "    \"\"\"\n",
    "    # read spectrum and construct binary table\n",
    "    channels, freq, flux_density, pixels = np.loadtxt(f_in, skiprows=38, unpack=True, usecols=[0,1,2,3])    \n",
    "    channels_col = fits.Column(name='Channel', format='D', array=channels.astype('int'), unit='')\n",
    "    freq_col = fits.Column(name='Frequency', format='E', array=freq, unit='Hz')\n",
    "    flux_density_col = fits.Column(name='Flux density', format='E', array=flux_density, unit='Jy')\n",
    "    pixel_col = fits.Column(name='Pixels', format='D', array=pixels.astype('int'), unit='')\n",
    "    fits_table = fits.BinTableHDU.from_columns([channels_col, freq_col, flux_density_col, pixel_col])\n",
    "    \n",
    "    # construct dummy image hdu\n",
    "    keys = ['OBJECT', 'CDELT1', 'CDELT2', 'CDELT3', 'CTYPE1', 'CTYPE2', 'CTYPE3', 'ORIGIN', 'EQUINOX', 'LONPOLE', 'LATPOLE', 'SRCVERS', 'SRCTR']\n",
    "    # keys += ['SBID']\n",
    "    with fits.open(f_cube, mode='readonly') as hdu_cube:\n",
    "        header_cube = hdu_cube[0].header\n",
    "    hdu = fits.PrimaryHDU()\n",
    "    header = hdu.header\n",
    "    hdu.data = np.array([[[0]]]).astype('int16')\n",
    "    header['CRPIX1'] = 0\n",
    "    header['CRPIX2'] = 0\n",
    "    header['CRPIX3'] = 0\n",
    "    header['CUNIT1'] = 'deg'\n",
    "    header['CUNIT2'] = 'deg'\n",
    "    header['CUNIT3'] = 'Hz'\n",
    "    header['CRVAL1'] = ra\n",
    "    header['CRVAL2'] = dec\n",
    "    header['CRVAL3'] = frequency\n",
    "    header['SPECSYS'] = 'BARYCENT'\n",
    "    header['RADESYS'] = 'FK5'\n",
    "    for k in keys:\n",
    "        header.set(k, header_cube[k])    \n",
    "\n",
    "    # construct hdulist and write to file\n",
    "    hdu_list = fits.HDUList([hdu, fits_table])\n",
    "    hdu_list.writeto(f_out, overwrite=True, output_verify='fix')\n",
    "    return\n",
    "\n",
    "def mom0_to_png(data, f_out):\n",
    "    \"\"\"Create plot of mom0 map as png file for archive cutouts\n",
    "\n",
    "    \"\"\"\n",
    "    plt.imshow(data)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(f_out, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc511fa-6a45-4cac-a751-edc228662c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update all product files\n",
    "\n",
    "product_tarfiles = glob.glob(os.path.join(release_name, '*.tar'))\n",
    "product_files = [f.replace('.tar', '') for f in product_tarfiles]\n",
    "\n",
    "# Extract\n",
    "for f in product_tarfiles:\n",
    "    filename = f.replace('.tar', '')\n",
    "    with tarfile.open(f) as tf:\n",
    "        tf.extractall(path=filename)\n",
    "    # os.remove(f)\n",
    "\n",
    "# Update product files\n",
    "for idx_pf, pf in enumerate(product_files):\n",
    "    print(f'Folder {pf} [{idx_pf + 1}/{len(product_files)}]')\n",
    "    fits_files = glob.glob(os.path.join(pf, '*.fits'))\n",
    "    for idx_ff, ff in enumerate(fits_files):\n",
    "        print(f'[{idx_ff + 1}/{len(fits_files)}] {ff}')\n",
    "        source_name = ff.split('/')[1]\n",
    "        with fits.open(ff, mode='update') as hdul:\n",
    "            header = hdul[0].header\n",
    "            # NOTE: DATE card?\n",
    "            header['SRCVERS'] = header['ORIGIN']  # Get SoFiA version from ORIGIN header\n",
    "            header['SRCTR'] = release_name\n",
    "            header['OBJECT'] = source_name\n",
    "            hdul.flush()\n",
    "\n",
    "            # Download moment 0 map figure\n",
    "            if 'mom0.fits' in ff:\n",
    "                print(f'[{idx_ff + 1}/{len(fits_files)}] Saving mom0 png figure')\n",
    "                data = hdul[0].data\n",
    "                mom0_png = ff.replace('.fits', '.png')\n",
    "                mom0_to_png(data, mom0_png)\n",
    "\n",
    "    # Update spectra\n",
    "    print('Creating spec.fits file')\n",
    "    spectra_files = glob.glob(os.path.join(pf, '*spec.txt'))\n",
    "    assert len(spectra_files) == 1, 'Should only be 1 spectrum file per detection'\n",
    "    spec_f_in = spectra_files[0]\n",
    "    spec_f_out = spec_f_in.replace('.txt', '.fits')\n",
    "    spec_f_cube = spec_f_in.replace('spec.txt', 'cube.fits')\n",
    "    assert os.path.exists(spec_f_cube), 'Cutout cube corresponding to spectra file does not exist'\n",
    "    row = write_table[write_table['name'] == source_name][0]\n",
    "    spectrum_to_fits(spec_f_in, spec_f_cube, spec_f_out, row['ra'], row['dec'], row['freq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba8ef59-aee9-4424-926b-dc6a7a000c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare header table\n",
    "\n",
    "header_table = write_table.copy()\n",
    "header_table.remove_rows(slice(0, len(header_table), 1))\n",
    "header_table\n",
    "header_votable = from_table(header_table)\n",
    "header_votable.version = '1.3'\n",
    "header_votable.to_xml(os.path.join(basedir, votable_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40361703-3b71-4791-b365-1d8c49605797",
   "metadata": {},
   "source": [
    "## CASDA release\n",
    "\n",
    "Move product files to required directory structure for CASDA public data releases:\n",
    "\n",
    "- catalogue (VOTable version 1.3)\n",
    "- cubelets (mask and cube files)\n",
    "- moment_maps (all moment maps, including a mom0 .png file if you want a preview)\n",
    "- spectra (.spec file in fits format)\n",
    "\n",
    "File formats are: `f'{WALLABY_name}_{release_version}.fits`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e0a0e4-2aad-4a24-a18b-6dcd1b451cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy files over for CASDA release\n",
    "\n",
    "# Create directory_structure\n",
    "basedir = os.path.join('CASDA', release_name)\n",
    "os.makedirs(os.path.join(basedir, 'catalogue'), exist_ok=True)\n",
    "os.makedirs(os.path.join(basedir, 'cubelets'), exist_ok=True)\n",
    "os.makedirs(os.path.join(basedir, 'moment_maps'), exist_ok=True)\n",
    "os.makedirs(os.path.join(basedir, 'spectra'), exist_ok=True)\n",
    "\n",
    "# Copy catalogue xml\n",
    "shutil.copy(votable_filename, os.path.join(basedir, 'catalogue', votable_filename))\n",
    "\n",
    "# Copy product files\n",
    "for idx_pf, pf in enumerate(product_files):\n",
    "    source_name = pf.split('/')[1].replace(' ', '_')\n",
    "    print(f'Source {source_name} [{idx_pf + 1}/{len(product_files)}]')\n",
    "    row = table[table['source_name'] == source_name.replace('_', ' ')][0]\n",
    "    srctr = row['SRCTR']\n",
    "    p_files = glob.glob(os.path.join(pf, '*'))\n",
    "    for f in p_files:\n",
    "        suffix = f.rsplit('_', 1)[1]\n",
    "        new_filename = f'{source_name}_{srctr}_{release_name}_{suffix}'\n",
    "        \n",
    "        # moment maps\n",
    "        if any([t in suffix for t in ['mom0', 'mom1', 'mom2', 'chan']]):\n",
    "            shutil.copy(f, os.path.join(basedir, 'moment_maps', new_filename))\n",
    "            \n",
    "        # cubelets\n",
    "        elif any([t in suffix for t in ['cube', 'mask']]):\n",
    "            shutil.copy(f, os.path.join(basedir, 'cubelets', new_filename))\n",
    "        \n",
    "        # spectra\n",
    "        elif any([t in suffix for t in ['spec.fits']]):\n",
    "            shutil.copy(f, os.path.join(basedir, 'spectra', new_filename))\n",
    "        \n",
    "        else:\n",
    "            print(f'Skipping file {f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee40c5f-436c-40ae-9f93-45f7d1215994",
   "metadata": {},
   "source": [
    "## CADC Release\n",
    "\n",
    "Create VOTable object for the metadata, but export the catalogue data as a CSV file. Copy product files for CADC public data release required file structure.\n",
    "\n",
    "- Each detection (file format: WALLABY name, release version) folder contains the product files\n",
    "- Each product file has the format: `f'{WALLABY_name}_{internal_release_version}_{release_version}_<ext>.fits`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cce822a-1245-471a-bc57-93c5b366468b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy files over for CADC release\n",
    "\n",
    "# Create directory_structure\n",
    "basedir = os.path.join('CADC', release_name)\n",
    "os.makedirs(basedir, exist_ok=True)\n",
    "\n",
    "# Write catalog\n",
    "ascii.write(write_table, os.path.join(basedir, votable_filename.replace('.xml', '.csv')), format='csv', overwrite=True)\n",
    "\n",
    "# Copy product files\n",
    "for idx_pf, pf in enumerate(product_files):\n",
    "    source_name = pf.split('/')[1].replace(' ', '_')\n",
    "    row = table[table['source_name'] == source_name.replace('_', ' ')][0]\n",
    "    srctr = row['SRCTR']\n",
    "    print(f'Source {source_name} [{idx_pf + 1}/{len(product_files)}]')\n",
    "    source_dir = os.path.join(basedir, f'{source_name}_{release_name}')\n",
    "    os.makedirs(source_dir, exist_ok=True)\n",
    "    p_files = glob.glob(os.path.join(pf, '*'))\n",
    "    for f in p_files:\n",
    "        suffix = f.rsplit('_', 1)[1]\n",
    "        new_filename = f'{source_name}_{srctr}_{release_name}_{suffix}'\n",
    "        if any([t in suffix for t in ['mom0', 'mom1', 'mom2', 'chan', 'cube', 'mask', 'spec.fits']]):\n",
    "            shutil.copy(f, os.path.join(source_dir , new_filename))\n",
    "        else:\n",
    "            print(f'Skipping file {f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978c81a8-85e5-49b2-8ddc-c97fe7a7ae8b",
   "metadata": {},
   "source": [
    "## Data Central\n",
    "\n",
    "Exporting to Data Central requires the generation of various .txt files that contain metadata for the survey and catalog files. Some user input here is required to describe the metadata for the project and the data release."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8502db2-b257-4e5e-a15b-b7d62010c736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survey description metadata\n",
    "\n",
    "import csv\n",
    "from datetime import datetime\n",
    "now = datetime.now().strftime('%d-%m-%Y')\n",
    "\n",
    "def dict_to_dc_meta(filename, data):\n",
    "    \"\"\"Custom function to write dicts to CSV files following the file format convention\n",
    "    required for Data Central metadata files.\n",
    "    \n",
    "    \"\"\"\n",
    "    with open(filename, 'w', newline='') as f:\n",
    "        csv_writer = csv.DictWriter(f, data.keys(), delimiter='|')\n",
    "        csv_writer.writeheader()\n",
    "        csv_writer.writerow(data)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2865161-c023-4b5f-ad51-3a49b1181333",
   "metadata": {},
   "source": [
    "<span style=\"font-weight: bold; color: #FF0000;\">⚠ Update the cell below with the relevant survey metadata for this release</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcb90af-7bfd-4568-8a76-b1d24904a8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survey metadata\n",
    "survey_meta = {\n",
    "    'name': 'wallaby',\n",
    "    'pretty_name': 'WALLABY',\n",
    "    'title': 'Widefield ASKAP L-band Legacy All-sky Blind surveY',\n",
    "    'description': 'The Widefield ASKAP L-band Legacy All-sky Blind surveY (or WALLABY) is one of a number of surveys that are now running on the Australian SKA Pathfinder (ASKAP), which is an innovative imaging radio telescope located in an extremely radio quiet zone (the Inyarrimanha Ilgari Bundara, Murchison Radio-astronomy Observatory) in Western Australia. The aim of WALLABY is to use the powerful widefield phased-array technology of ASKAP to observe half of the Southern Hemisphere in the 21-cm line of neutral hydrogen (or HI) at 30-arcsec resolution (with a simultaneous 10-arcsec zoom mode for previously-known galaxies), thereby detecting and imaging the gas distribution in hundreds of thousands of external galaxies in the local Universe. This will allow astronomers to gain a much improved understanding of the processes involved in galaxy formation and evolution, and the role of stellar and black hole feedback, gas accretion and galaxy interactions in these processes. WALLABY has concluded two Pilot Survey phases and has imaged nearly 400 square degree of sky around nearby galaxy clusters with the full ASKAP-36 array, as well as a number of early science fields with smaller numbers of antennas. Full WALLABY started in late-2022.',\n",
    "    'pi': 'Lister Staveley-Smith, Barbara Catinella',\n",
    "    'contact': 'lister.staveley-smith@uwa.edu.au',\n",
    "    'website': 'https://wallaby-survey.org/'\n",
    "}\n",
    "\n",
    "# Release meta\n",
    "release_meta = {\n",
    "    'name': 'wallaby_pdr2',\n",
    "    'pretty_name': 'WALLABY Pilot Survey public data release 2',\n",
    "    'version': 1,\n",
    "    'data_release_number': 1,\n",
    "    'contact': 'Tobias Westmeier <tobias.westmeier@uwa.edu.au>',\n",
    "    'group': 'WALLABY', \n",
    "    'public': True\n",
    "}\n",
    "\n",
    "# Catalogue metadata\n",
    "group_meta = {\n",
    "    'name': 'WALLABY DR2',\n",
    "    'pretty_name': 'WALLABY DR2',\n",
    "    'description': 'WALLABY Pilot Survey Public Data Release 2',\n",
    "    'documentation': '-',\n",
    "    'contact': 'Tobias Westmeier <tobias.westmeier@uwa.edu.au>',\n",
    "    'date': now,\n",
    "    'version': 1\n",
    "}\n",
    "\n",
    "coordinate_meta = {\n",
    "    'table_name': 'detection',\n",
    "    'source_name_col': 'name',\n",
    "    'long_col': 'ra',\n",
    "    'lat_col': 'dec',\n",
    "    'long_format': 'deg',\n",
    "    'lat_format': 'deg',\n",
    "    'frame': 'fk5',\n",
    "    'equinox': 'J2000'\n",
    "}\n",
    "\n",
    "sql_meta = {\n",
    "    'table_name': 'detection',\n",
    "    'sql': '\"SELECT * FROM detection\"'\n",
    "}\n",
    "\n",
    "table_meta = {\n",
    "    'name': 'detection',\n",
    "    'description': 'WALLABY Pilot Survey Public Data Release 2',\n",
    "    'group_name': 'WALLABY',\n",
    "    'filename': '',\n",
    "    'contact': 'Tobias Westmeier <tobias.westmeier@uwa.edu.au>',\n",
    "    'date': now,\n",
    "    'version': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19e0b0b-389b-4667-9014-a8e3d83b035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survey metadata\n",
    "\n",
    "basedir = os.path.join('data_central', 'wallaby')\n",
    "os.makedirs(basedir, exist_ok=True)\n",
    "dict_to_dc_meta(os.path.join(basedir, 'wallaby_survey_meta.txt'), survey_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603f36ca-7e52-421b-b687-e1187430f33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release metadata\n",
    "\n",
    "release_dir = os.path.join(basedir, release_name.lower())\n",
    "os.makedirs(release_dir, exist_ok=True)\n",
    "dict_to_dc_meta(os.path.join(release_dir, f'{release_name.lower()}_release_meta.txt'), release_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f22fc2-ab61-4d03-baf1-59da898817c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catalogue metadata\n",
    "\n",
    "catalogues_dir = os.path.join(release_dir, 'catalogues')\n",
    "os.makedirs(catalogues_dir, exist_ok=True)\n",
    "\n",
    "catalogues_meta = {'name': None, 'table_name': None, 'description': None, 'ucd': None, 'unit': None, 'data_type': None}\n",
    "with open(os.path.join(catalogues_dir, f'{release_name.lower()}_column_meta.txt'), 'w', newline='') as f:\n",
    "    csv_writer = csv.DictWriter(f, catalogues_meta.keys(), delimiter='|')\n",
    "    csv_writer.writeheader()\n",
    "    for field in header_votable.get_first_table().iter_fields_and_params():\n",
    "        csv_writer.writerow({\n",
    "            'name': field.ID,\n",
    "            'table_name': field.ID,\n",
    "            'description': field.description or '',\n",
    "            'ucd': field.ucd or '',\n",
    "            'unit': field.unit or '',\n",
    "            'data_type': field.datatype\n",
    "        })\n",
    "\n",
    "dict_to_dc_meta(os.path.join(catalogues_dir, f'{release_name.lower()}_coordinate_meta.txt'), coordinate_meta)\n",
    "dict_to_dc_meta(os.path.join(catalogues_dir, f'{release_name.lower()}_group_meta.txt'), group_meta)\n",
    "dict_to_dc_meta(os.path.join(catalogues_dir, f'{release_name.lower()}_sql_meta.txt'), sql_meta)\n",
    "dict_to_dc_meta(os.path.join(catalogues_dir, f'{release_name.lower()}_table_meta.txt'), table_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e922359-5851-43f5-b95a-8af87277e62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# astroobjects metadata\n",
    "\n",
    "astroobjects_dir = os.path.join(release_dir, 'astroobjects')\n",
    "os.makedirs(astroobjects_dir, exist_ok=True)\n",
    "\n",
    "astroobjects_header = {'source_name': None, 'ra': None, 'dec': None}\n",
    "with open(os.path.join(astroobjects_dir, f'{release_name.lower()}_astroobjects.txt'), 'w', newline='') as f:\n",
    "    csv_writer = csv.DictWriter(f, astroobjects_header.keys(), delimiter='|')\n",
    "    csv_writer.writeheader()\n",
    "    for row in write_table:\n",
    "        csv_writer.writerow({\n",
    "            'source_name': row['name'],\n",
    "            'ra': row['ra'],\n",
    "            'dec': row['dec']\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a001e93d-e839-4099-a878-fdded78497d6",
   "metadata": {},
   "source": [
    "# 3. Kinematic models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a03a19-0ed3-40ce-b93e-2821378c5701",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165f88eb-2628-4da6-b9d3-01eb6e29c28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export kinematic models\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d788b113-95be-4f83-9a1a-8646e8cf71bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
