{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "935cdfea",
   "metadata": {},
   "source": [
    "# WALLABY public data release notebook\n",
    "\n",
    "This notebook is intended to support with exporting the WALLABY source and kinematic data tables and associated products to be ingested into a public archive (CADC and CASDA). It is intended that the user of this notebook will be a member of the WALLABY project team, or a member of the WALLABY TWG7 group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd0cc98-4b05-447c-aa33-3d27a84251d1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5630c448-e003-4ae0-9efc-bfef293568a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import getpass\n",
    "import requests\n",
    "import getpass\n",
    "import pyvo as vo\n",
    "from pyvo.auth import authsession, securitymethods\n",
    "import numpy as np\n",
    "from astropy.io.votable import from_table, parse_single_table\n",
    "from astropy.table import vstack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a7a9ff-8084-4ad1-9d2c-d0d8aa62dab6",
   "metadata": {},
   "source": [
    "### Authenticate\n",
    "\n",
    "<span style=\"font-weight: bold; color: #FF0000;\">⚠ Update the cell below with your username and enter your password</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4220e4-57ba-49e5-be19-983caf994019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter WALLABY user username and password\n",
    "\n",
    "username = 'wallaby_user'\n",
    "password = getpass.getpass('Enter your password')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02fa60a-d21d-483f-82a0-2862322e57e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect with TAP service\n",
    "\n",
    "URL = \"https://wallaby.aussrc.org/tap\"\n",
    "auth = vo.auth.AuthSession()\n",
    "auth.add_security_method_for_url(URL, vo.auth.securitymethods.BASIC)\n",
    "auth.credentials.set_password(username, password)\n",
    "tap = vo.dal.TAPService(URL, session=auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a27862-d912-4d6f-ad9d-82ccbb3aad9b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9800389-c6bd-42f3-89eb-a484dcf62118",
   "metadata": {},
   "source": [
    "# 1. Decide Release\n",
    "\n",
    "Determine which internal releases you would like to bundle in this public data release. You will also need to set a name for this public data release."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bb2bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all tags\n",
    "\n",
    "query = \"SELECT * FROM wallaby.tag\"\n",
    "votable = tap.search(query)\n",
    "table = votable.to_table()\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8bc157-fa15-4266-b52a-22b9e1127475",
   "metadata": {},
   "source": [
    "<span style=\"font-weight: bold; color: #FF0000;\">⚠ Update the cell below. Add tags to the list for release, and update `release_name` variable</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e633128b-9114-4937-8d29-7ec0294bbfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of tags\n",
    "tags = ['Norma DR1', 'NGC 5044 DR2']\n",
    "\n",
    "# Release name\n",
    "release_name = \"WALLABY Test PDR\"\n",
    "release_name = release_name.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc709bf5-0ced-485a-a750-6f50229579dd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8af3de",
   "metadata": {},
   "source": [
    "# 2. Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0d9803-88d3-4734-bd38-2202c26d2d47",
   "metadata": {},
   "source": [
    "## Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac09df6-aa7a-4069-83c1-01946cfe2057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve catalog as Astropy table\n",
    "\n",
    "query = \"\"\"SELECT d.*, ivo_string_agg(t.name || ': ' || t.description, '; ') AS tags, ivo_string_agg(c.comment, '; ') AS comments\n",
    "        FROM wallaby.detection d\n",
    "        LEFT JOIN wallaby.tag_detection td ON d.id = td.detection_id \n",
    "        LEFT JOIN wallaby.tag t ON t.id = td.tag_id\n",
    "        LEFT JOIN wallaby.comment c ON d.id = c.detection_id\n",
    "        WHERE t.name IN ('Internal Data Release', '$TAG_NAME')\n",
    "        GROUP BY d.id\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807033dd-7a35-4c13-953f-6e93238b0436",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "table = None\n",
    "for idx, tag_name in enumerate(tags):\n",
    "    q = query.replace('$TAG_NAME', tag_name)\n",
    "    result = tap.search(q)\n",
    "    if idx == 0:\n",
    "        table = result.to_table()\n",
    "    else:\n",
    "        table = vstack([table, result.to_table()])\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afd86a0-764f-461c-875b-24c5cdbe0a60",
   "metadata": {},
   "source": [
    "## Modifying the catalog\n",
    "\n",
    "There are some additional columns and calculated properties that are required for the release. The column metadata (e.g. UCDs, units, description etc as required to conform with VO standards) also need to be included for these additional columns. These include:\n",
    "\n",
    "| Column | Description |\n",
    "| --- | --- |\n",
    "| `qflag` |  |\n",
    "| `kflag` | column to indicate whether or not there is a kinematic model associated with the detection |\n",
    "| `team_release` | |\n",
    "| `f_sum_corr` | |\n",
    "| `err_f_sum_corr` | |\n",
    "| `dist_h` | |\n",
    "| `log_m_hi_corr` | Uses `v_est` and `dist_est` which are calculated properties |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa29d2a-ae43-4253-81ee-39ab102e6d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table corrections\n",
    "\n",
    "rest_freq = 1.42040575179E+09\n",
    "c = 2.9979245e8\n",
    "H0 = 70.0\n",
    "\n",
    "table['name'] = table['source_name']\n",
    "table['qflag'] = table['flag']\n",
    "table['kflag'] = np.zeros(len(table['flag']))\n",
    "table['team_release'] = ['' * len(table['flag'])]\n",
    "table['f_sum_corr'] = table['f_sum'] / 10.0 ** (0.0285 * np.log10(table['f_sum'])**3.0 -0.439 * np.log10(table['f_sum'])**2.0 + 2.294 * np.log10(table['f_sum']) - 4.097)\n",
    "table['err_f_sum_corr'] = table['err_f_sum'] / table['f_sum'] * table['f_sum_corr']\n",
    "table['v_est'] = ((rest_freq - table['freq']) / table['freq'] * c / 1000.0)\n",
    "table['dist_h'] = table['v_est'] / H0\n",
    "table['log_m_hi'] = np.log10(49.7 * table['dist_h']**2.0 * table['f_sum'])\n",
    "table['log_m_hi_corr'] = np.log10(49.7 * table['dist_h']**2.0 * table['f_sum_corr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833b22d6-fa82-46e7-8da9-8400ff61ee9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove certain columns from the astropy table\n",
    "\n",
    "write_table = table\n",
    "write_table.remove_columns(['id', 'run_id', 'instance_id', 'access_url', 'access_format', 'source_name', 'flag', 'v_est', 'l', 'b', 'v_rad', 'v_opt', 'v_app', 'tags'])\n",
    "votable = from_table(write_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd385ff-dbfb-4f59-9fcd-c354d6591be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update derived quantity columns of votable\n",
    "\n",
    "f_sum_corr_field = votable.get_field_by_id('f_sum_corr')\n",
    "f_sum_corr_field.ucd = \"phot.flux;meta.main\"\n",
    "f_sum_corr_field.unit = \"Jy*Hz\"\n",
    "f_sum_corr_field.description = \"The integrated flux within 3D source mask statistically corrected to match single dish observations\"\n",
    "\n",
    "err_f_sum_corr_field = votable.get_field_by_id('err_f_sum_corr')\n",
    "err_f_sum_corr_field.ucd = \"stat.error;phot.flux\"\n",
    "err_f_sum_corr_field.unit = \"Jy*Hz\"\n",
    "err_f_sum_corr_field.description = \"Statistical uncertainty of the single dish corrected integrated flux\"\n",
    "\n",
    "dist_h_field = votable.get_field_by_id('dist_h')\n",
    "dist_h_field.ucd = \"pos.distance\"\n",
    "dist_h_field.unit = \"Mpc\"\n",
    "dist_h_field.description = \"Local Hubble distance derived from the barycentric source frequency\"\n",
    "\n",
    "log_m_hi_field = votable.get_field_by_id('log_m_hi')\n",
    "log_m_hi_field.ucd = \"phys.mass\"\n",
    "log_m_hi_field.unit = \"log10(Msol)\"\n",
    "log_m_hi_field.description = \"The estimated log10 mass of the cube using f_sum and freq\"\n",
    "\n",
    "log_m_hi_corr_field = votable.get_field_by_id('log_m_hi_corr')\n",
    "log_m_hi_corr_field.ucd = \"phys.mass\"\n",
    "log_m_hi_corr_field.unit = \"log10(Msol)\"\n",
    "log_m_hi_corr_field.description = \"The estimated log10 mass of the cube using f_sum_corr and freq\"\n",
    "\n",
    "qflag_field = votable.get_field_by_id('qflag')\n",
    "qflag_field.datatype = \"double\"\n",
    "qflag_field.ucd = \"meta.code.qual\"\n",
    "qflag_field.description = \"Quality flag\"\n",
    "\n",
    "kflag_field = votable.get_field_by_id('kflag')\n",
    "kflag_field.datatype = \"double\"\n",
    "kflag_field.ucd = \"meta.code\"\n",
    "kflag_field.description = \"Kinematic model flag\"\n",
    "\n",
    "comments_field = votable.get_field_by_id('comments')\n",
    "comments_field.datatype = \"char\"\n",
    "comments_field.ucd = \"meta.note\"\n",
    "comments_field.description = \"Comments on individual sources\"\n",
    "\n",
    "team_release_field = votable.get_field_by_id('team_release')\n",
    "team_release_field.datatype = \"char\"\n",
    "team_release_field.ucd = \"meta.dataset;meta.main\"\n",
    "team_release_field.description = \"Internal team release identifier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319e31d3-a350-4132-b3e3-42e2003a006c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(write_table.columns)\n",
    "print(len(write_table.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9b253d-c993-4894-9361-baa0934a3b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download catalog table\n",
    "\n",
    "votable.version = '1.3'\n",
    "votable_filename = f'{release_name}_SourceCatalogue.xml'\n",
    "votable.to_xml(votable_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5711c1-928d-4a93-918e-355d5949a735",
   "metadata": {},
   "source": [
    "## Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9179ec6d-9ed5-468d-9411-cc4e5746b081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful function for downloading table products (requires authentication)\n",
    "\n",
    "def download_products(row, products_filename, chunk_size=8192):\n",
    "    \"\"\"Download products for a row of the table (a detection entry)\n",
    "    \n",
    "    \"\"\"\n",
    "    name = row['source_name']\n",
    "    access_url = row['access_url']\n",
    "    votable = parse_single_table(access_url)\n",
    "    product_table = votable.to_table()\n",
    "    url = product_table[product_table['description'] == 'SoFiA-2 Detection Products'][0]['access_url']\n",
    "    with requests.get(url, auth=(username, password), stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(products_filename, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                f.write(chunk)\n",
    "    print(f'Downloaded completed for {name}')\n",
    "    return\n",
    "\n",
    "def download_table_products(table, directory, chunk_size=8192):\n",
    "    \"\"\"Download WALLABY products from ADQL queried table\n",
    "\n",
    "    \"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.mkdir(directory)\n",
    "    print(f'Saving products to {directory}')\n",
    "    for row in table:\n",
    "        name = row['source_name']\n",
    "        products_filename = os.path.join(directory, f'{name}.tar')\n",
    "        download_products(row, products_filename, chunk_size)\n",
    "    print('Downloads complete')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278faf65-85e4-445b-b51c-dcd2540832bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write output products for a source\n",
    "\n",
    "download_table_products(table[0:3], release_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596ccb3c-140b-4321-8ce2-b31bc17da0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update product files\n",
    "\n",
    "import tarfile\n",
    "import glob\n",
    "from astropy.io import fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc511fa-6a45-4cac-a751-edc228662c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update all product files\n",
    "\n",
    "product_tarfiles = glob.glob(os.path.join(release_name, '*.tar'))\n",
    "product_files = [f.replace('.tar', '') for f in product_tarfiles]\n",
    "\n",
    "# Extract\n",
    "for f in product_tarfiles:\n",
    "    filename = f.replace('.tar', '')\n",
    "    with tarfile.open(f) as tf:\n",
    "        tf.extractall(path=filename)\n",
    "    # os.remove(f)\n",
    "\n",
    "# Update fits files\n",
    "for idx_pf, pf in enumerate(product_files):\n",
    "    print(f'Folder {pf} [{idx_pf + 1}/{len(product_files)}]')\n",
    "    fits_files = glob.glob(os.path.join(pf, '*.fits'))\n",
    "    for idx_ff, ff in enumerate(fits_files):\n",
    "        print(f'[{idx_ff + 1}/{len(fits_files)}] {ff}')\n",
    "        source_name = ff.split('/')[1]\n",
    "        with fits.open(ff, mode='update') as hdul:\n",
    "            header = hdul[0].header\n",
    "            # NOTE: DATE card?\n",
    "            header['SRCVERS'] = header['ORIGIN']  # Get SoFiA version from ORIGIN header\n",
    "            header['SRCTR'] = release_name\n",
    "            header['OBJECT'] = source_name\n",
    "            hdul.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40361703-3b71-4791-b365-1d8c49605797",
   "metadata": {},
   "source": [
    "## CASDA release\n",
    "\n",
    "Move product files to required directory structure for CASDA public data releases:\n",
    "\n",
    "- catalogue (VOTable version 1.3)\n",
    "- cubelets (mask and cube files)\n",
    "- moment_maps (all moment maps, including a mom0 .png file if you want a preview)\n",
    "- spectra (.spec file in fits format)\n",
    "\n",
    "File formats are: `f'{WALLABY_name}_{release_version}.fits`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e0a0e4-2aad-4a24-a18b-6dcd1b451cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy files over for CASDA release\n",
    "\n",
    "# Create directory_structure\n",
    "basedir = os.path.join('CASDA', release_name)\n",
    "os.makedirs(os.path.join(basedir, 'catalogue'), exist_ok=True)\n",
    "os.makedirs(os.path.join(basedir, 'cubelets'), exist_ok=True)\n",
    "os.makedirs(os.path.join(basedir, 'moment_maps'), exist_ok=True)\n",
    "os.makedirs(os.path.join(basedir, 'spectra'), exist_ok=True)\n",
    "\n",
    "# Copy catalogue xml\n",
    "shutil.copy(votable_filename, os.path.join(basedir, 'catalogue', votable_filename))\n",
    "\n",
    "# Copy product files\n",
    "for idx_pf, pf in enumerate(product_files):\n",
    "    source_name = pf.split('/')[1].replace(' ', '_')\n",
    "    print(f'Source {source_name} [{idx_pf + 1}/{len(product_files)}]')\n",
    "    p_files = glob.glob(os.path.join(pf, '*'))\n",
    "    for f in p_files:\n",
    "        suffix = f.rsplit('_', 1)[1]\n",
    "        new_filename = f'{source_name}_{release_name}_{suffix}'\n",
    "        \n",
    "        # moment maps\n",
    "        if any([t in suffix for t in ['mom0', 'mom1', 'mom2', 'chan']]):\n",
    "            shutil.copy(f, os.path.join(basedir, 'moment_maps', new_filename))\n",
    "            \n",
    "        # cubelets\n",
    "        elif any([t in suffix for t in ['cube', 'mask']]):\n",
    "            shutil.copy(f, os.path.join(basedir, 'cubelets', new_filename))\n",
    "        \n",
    "        # spectra\n",
    "        elif any([t in suffix for t in ['spec']]):\n",
    "            shutil.copy(f, os.path.join(basedir, 'spectra', new_filename))\n",
    "        \n",
    "        else:\n",
    "            print(f'Skipping file {f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee40c5f-436c-40ae-9f93-45f7d1215994",
   "metadata": {},
   "source": [
    "## CADC Release\n",
    "\n",
    "Copy product files for CADC public data release required file structure.\n",
    "\n",
    "- Each detection (file format: WALLABY name, release version) folder contains the product files\n",
    "- Each product file has the format: `f'{WALLABY_name}_{release_version}_<ext>.fits`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cce822a-1245-471a-bc57-93c5b366468b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy files over for CADC release\n",
    "\n",
    "# Create directory_structure\n",
    "basedir = os.path.join('CADC', release_name)\n",
    "os.makedirs(basedir, exist_ok=True)\n",
    "\n",
    "# Copy catalogue xml\n",
    "shutil.copy(votable_filename, os.path.join(basedir, votable_filename))\n",
    "\n",
    "# Copy product files\n",
    "for idx_pf, pf in enumerate(product_files):\n",
    "    source_name = pf.split('/')[1].replace(' ', '_')\n",
    "    print(f'Source {source_name} [{idx_pf + 1}/{len(product_files)}]')\n",
    "    source_dir = os.path.join(basedir, f'{source_name}_{release_name}')\n",
    "    os.makedirs(source_dir, exist_ok=True)\n",
    "    p_files = glob.glob(os.path.join(pf, '*'))\n",
    "    for f in p_files:\n",
    "        suffix = f.rsplit('_', 1)[1]\n",
    "        new_filename = f'{source_name}_{release_name}_{suffix}'\n",
    "        if any([t in suffix for t in ['mom0', 'mom1', 'mom2', 'chan', 'cube', 'mask', 'spec']]):\n",
    "            shutil.copy(f, os.path.join(source_dir , new_filename))\n",
    "        else:\n",
    "            print(f'Skipping file {f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a001e93d-e839-4099-a878-fdded78497d6",
   "metadata": {},
   "source": [
    "# 3. Kinematic models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a03a19-0ed3-40ce-b93e-2821378c5701",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165f88eb-2628-4da6-b9d3-01eb6e29c28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export kinematic models\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d788b113-95be-4f83-9a1a-8646e8cf71bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
